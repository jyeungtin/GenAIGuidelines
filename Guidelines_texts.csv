Continent,Country,Name,Type,Title,Text,Release_date,Link
Europe,Germany,Bayerischer Rundfunk,PBC,Ethics of Artificial Intelligence: Our AI Ethics Guidelines,"User Benefit We deploy AI to help us use the resources that our users entrust us with more responsibly by making our work more efficient. We also use AI to generate new content, develop new methods for investigative journalism and make our products more attractive to our users. Transparency and Discourse. We participate in the debate on the societal impact of algorithms by providing information on emerging trends, investigating algorithms, explaining how technologies work and strengthening an open debate on the future role of public service media in a data society. We study the AI ethics discourse in other institutions, organizations and companies in order to check and improve our guidelines to avoid a gap between theory and practice. We make plain for our users what technologies we use, what data we process and which editorial teams or partners are responsible for it. When we encounter ethical challenges in our research and development, we make them a topic in order to raise awareness for such problems and make our own learning process transparent. Diversity and Regional Focus We embark on new projects conscious of the societal diversity of Bavaria and Germany. For instance, we strive towards dialect models in speech to text applications and bias-free training data (algorithmic accountability). We work with Bavarian startups and universities to make use of the AI competence in the region and support the community through use cases in the media industry and academia. We do strive for the utmost reliability in our operations and might chose to work with established tech companies on a case-by-case basis. Where possible, we work within our networks of ARD and the European Broadcasting Union (EBU), and consciously bring the ethical aspects of any proposed application to the collaboration. Conscious Data Culture We require solid information about their data sources from our vendors: What data was used to train the model? Correspondingly, we strive for integrity and quality of training data in all in-house development, especially to prevent algorithmic bias in the data and render visible the diversity of society. We continually raise awareness amongst our employees for the value of data and the importance of well-kept metadata. For only reliable data can produce reliable AI applications. A conscious data culture is vital to our day-to-day work and an important leadership task to future-proof public service media. We collect as little data as possible (data avoidance) and as much data as necessary (data economy) to fulfill our democratic mandate. We continue to uphold high data security standards and raise awareness for the responsible storage, processing and deletion of data, especially when it concerns personal data. We design the user experience of our media services with data sovereignty for the user in mind. Responsible Personalization Personalization can strengthen the information and entertainment value of our media services, so long as it does not undermine societal diversity and prevents unintended filter bubble effects. Hence, we use data-driven analytics as assistive tools for editorial decision-making. And in order to develop public service minded recommendation engines, we actively collaborate with other European media services through EBU. Editorial Control While the prevalence of data and automation introduce new forms of journalism, the editorial responsibility remains with the editorial units. The principle of editorial checks continues to be mandatory, even with automated content. But its implementation changes: the check of every individual piece of content is replaced by a plausibility check of causal structures in the data and a rigorous integrity examination of the data source. Agile Learning To continuously improve products and guidelines, we need experience and learning from pilot projects and prototypes. Experiments are an explicit part of this process. Up until and including the beta phase, these guidelines offer general orientation. In the final release candidate phase, they are fully binding. That way, we ensure that our final product offering fulfills the highest standards, while still encouraging a culture of learning and experimentation in our day-to-day work. We also pledge to listen to our users, invite their feedback and adjust our services if necessary. Partnerships We offer a practical research context for students and faculty at universities and collaborate with academia and industry to run experiments, for example with machine learning models and text generation. We exchange ideas with research institutions and ethics experts. Talent and Skill Acquisition Given the dynamic technology spectrum in the field of AI, we proactively ensure that BR has sufficient employees with the skills to implement AI technologies at the cutting edge of the industry in a responsible, human-centric way. We aim to recruit talent of diverse backgrounds with practical AI skills which we encourage them to deploy towards public service journalism. Interdisciplinary Reflection Instead of running ethics reviews after significant resources are invested, we integrate the interdisciplinary reflection with journalists, developers and management from the beginning of the development pipeline. That way, we ensure that no resources are wasted on projects that predictably do not meet these guidelines. We reflect on ethical red flags in our use of AI technologies regularly and in interdisciplinary fashion. We evaluate these experiences in light of the German public service media mandate and these ethics guidelines.",30-11-2020,https://www.br.de/extra/ai-automation-lab-english/ai-ethics100.html
Europe,Germany,DPA,Agency,The 5 AI guidelines of the DPA,"The DPA uses AI for various purposes and is open to the increased use of AI. AI will help to do our work better and faster - always in the interest of our customers and our products. The DPA only uses AI under human supervision. The final decision about the use of AI-based products is made by a human. We respect human autonomy and the primacy of human choices. The DPA only uses legitimate AI that complies with applicable law and statutory provisions and that meets our ethical principles, such as human autonomy, fairness and democratic values. The DPA uses AI that is technically robust and secure to minimize the risk of errors and misuse. Where content is generated exclusively by AI, we make this transparent and explainable. A person is always responsible for all content generated with AI. The DPA encourages all employees to be open and curious about the possibilities of AI, to test tools and to make suggestions for use in our workflows. Transparency, openness and documentation are crucial.",03-04-2023,https://innovation.dpa.com/2023/04/03/kuenstliche-intelligenz-fuenf-guidelines-der-dpa/
Europe,The Netherlands,ANP,Agency,Guideline: this is how the ANP editors deal with AI,"Like previous automations (the telegraph, the telex, the computer, the fax, the Internet, the telephone, search engines, social media...) artificial intelligence (""AI"") will change our journalistic work in ways that we, as ANP editors, cannot yet foresee: some innovations facilitate our daily work significantly and take us both journalistically and professionally rapidly further, others may affect our principles including our commitment to quality and reliability.

Therefore, we hereby establish basic principles on how the ANP editorially handles AI or derived artificial editorial applications including but not limited to so-called 'generative AI', 'templating' or 'text-to-speech' and 'speech-to-text'. Simply put, this is the line we follow. This is a living document that can be modified by the editor-in-chief as the developments call for it.

As with everything, in our thinking and work we first refer back to the Editorial Statutes. These state, among other things, that the purpose of ANP for example is ""to maintain a completely impartial and independent bureau for the objective provision of completely impartial and independent bureau for the objective supply of domestic and foreign news reports to the Dutch media"". For the journalistic policy, the articles of association keep us informed, include ""no direct influence by anyone, neither from outside nor from within, other than in the manner provided for in this Statute,"" ""impartiality and care in the presentation of verifiable facts and opinions"" and ""completeness and balance in the selection from relevant sources"" our maxims.

This is what we have to adhere to. In line with this, the following AI principles established by the chief editors and endorsed by the chiefs and editorial board. Violation of these is considered contrary to our basic journalistic principles and comparable to committing plagiarism.

1. We rely at all times on the Editorial Statutes, the law when deploying AI, the democratic principles that apply in the Netherlands and the mores of the journalistic profession, regardless of the possibilities that technical developments offer us, in the interest of the customers, our editorial standards and journalistic social authority. We are convinced that the ANP is best served by this.

2. As we look at society, every member of the ANP editorial staff looks at AI and related systems: full of wonder, inquisitive, critical and open to developments.

3. We see many opportunities with this and can be inspired by the computer (or by AI) to be inspired for such things as headlines, backgrounds, story ideas or sources to be accessed. Also - for example - recorded texts can be automatically transcribed and other support services applied. The number of possibilities for assistance is bound to increase further. It is up to editors whether it is relevant and useful whether AI applications are used in editorial or journalistic productions are deployed. Simply put: AI is, as it appears, a tool and not a substitute.
4. We can use AI or similar systems to support final editing, provided a human does a final check afterwards.

5. We continue to innovate on the principle of ""cautious experimentation. We want to actively pursue innovations and efficient work processes, but do not show ourselves to be reckless in doing so. Editorial innovations on journalistic productions for which the editor-in-chief are presented to the public only when they presented when they meet the principles of this document and the quality requirements applicable within ANP editorial staff.

6. We publish text, images, graphics or audio generated in any way by the computer (or by AI) is generated only with permission from chiefs and chief editors. We are both investigative and cautious in this regard because AI texts may look as promising as they may contain errors (""hallucinations"") and may reaffirm biases (""bias""). Moreover it is complex with computer-generated content to assess the reliability of the facts presented as true. In this consideration of publishing, we include how the AI system in question arrived at the information arrived at.

7. In our production chain we stick to the line already in place man>machine>human in which thinking and decision making begins and ends, possibly in intermediate production assisted by the computer. We therefore do not use computer (or AI) generated content, even as source material, without the checking of this information by a human being. After all, we remain, each of us remains responsible for ANP's reliability. We start with the facts (and that's where we end).

8. We are open to our customers about our methods and the deployment of AI or other technical systems. As a journalistic service provider, we strive for transparency, in that line we state where we as editors consider it deem appropriate the extent of AI use. Of course we are always available for customers always available for questions or explanations.

If there are questions to which the above principles do not answer, we will consult with you. This way we avoid surprises afterwards.",01-03-2023,https://www.elger.fm/content/files/2023/04/Leidraad_-zo-gaat-de-ANP-redactie-om-met-AI--4_2023-.pdf
Europe,The Netherlands,MediaHuis,News company,An AI Framework for MediaHuis,"Augment, Not Replace. We carefully consider AI's role in the creative process. AI should enhance journalism and support newsroom personnel. By utilizing AI to automate time-consuming tasks, the newsroom should be able to focus on areas where expertise and critical judgment truly matter. Ultimately, we seek to enhance the overall quality of our journalism for the audience.

Transparency Above All. We always disclose when AI is used to create or modify content (e.g. summaries) or alter reader experience (e.g. personalization), using disclaimers, labels, or watermarks as needed. We publish our guidelines on how we use AI and are transparent about how we apply AI in our tools, products, and journalistic output. We encourage readers to give feedback on AI applications and allow them to review their personal data that we use in our AI decisions.

Human in the Loop. We do not publish AI-generated or modified content without a ‘human in the loop’ in the course of the publishing process. Ultimately, the editor-in-chief remains responsible for all content, including content that is generated or automated with the help of AI tools. The editor-in-chief oversees the application and implementation of AI technologies in the newsroom in order to ensure it adheres to existing journalism codes and legal/ethical standards. For every AI-related newsroom process there should be a designated key contact: In the newsroom to answer any editorial or ethical questions, and at TPS to monitor and ensure AI systems work correctly and in line with laws and regulations.

Be Fair and Without Bias. We are vigilant about potential biases in AI systems discriminating against groups of people based on race, gender, ethnicity, religion, age, or otherwise. We acknowledge that current AI tools may be prone to error and bias and consider this when we seek to develop proprietary AI models and tools. We ensure a balance of journalistic, commercial, and audience value objectives in the application of AI. Personalization should provide extra value without leading to filter bubbles or inaccessible content. Organizing news and providing hierarchy and overview are fundamental tasks of good journalism.

Trust is Key. Being a trusted source will be even more crucial to the societal role and task of news media, especially in a world where false news and fake images are easily created and spread. We should follow and reinforce the rules for verifying sources to prevent the unintentional distribution of harmful AI-generated content. We also see it as our mission to instill a healthy sense of critical skepticism in our readers and provide them with tools and knowledge to assess trustworthiness.  We respect copyright, particularly when AI elaborates on or imitates a recognizable style, content, approach or imagery from human creators.  

Privacy and Security as Priority. When using AI applications, we prioritize data privacy and security. We collect only necessary data, adhere to privacy laws, and where required obtain user consent before using personal information. We regularly check data for accuracy and biases, and securely store and delete it when required. By complying with relevant laws and regulations, we protect personal data from unauthorized access and have a process in place to address any data breaches or security incidents.  We clearly explain how we use personal data in AI applications and empower users to control their information, including accessing, modifying, or deleting their data. 
AI Training and Skills. We have clear lines of accountability for developing, deploying, and using AI in our products and services. We make sure people responsible for AI decisions are properly trained and qualified. We invest in awareness and communication in the newsrooms so our journalists are informed of and trained in acceptable AI content practices.",14-06-2023,https://www.independent.ie/editorial/editorial/aiframework140623.pdf
Europe,Switzerland,Heidi.News,Online multimedia company,The editorial staff of Heidi.news takes a position on the use of artificial intelligence,"General principles. The rise of generative AI does not call into question the ethical principles of the profession, as contained in the Munich declaration of 1971, the Tunis declaration of 2019, and the Swiss declaration of the duties and rights of journalists. Editorial staff can use AI to facilitate or improve their work, but human intelligence remains at the heart of all our editorial production. No content will be published without prior human supervision. Heidi.news supports the work of journalists, authors, photographers and illustrators. Our media does not intend to replace them with machines. Synthetic texts. Any article published is signed by one or more journalists, who remain guarantors of the veracity and relevance of the information it contains. Artificial intelligence can help the work of journalists for refining raw data in the same way as software like Excel, and as a supplement for writing articles, like online thesaurus or proofreaders. automatic spelling. AIs are tools, but not sources of information. Synthetic images. The editorial staff limits itself to using the synthetic images for illustrative purposes, and not for information, so as not to induce confusion as to events in the real world. Heidi.news will not publish a synthetic image that could pass for a photograph, except for educational purposes, when the image in question is already public. Any synthetic image published will be accompanied by a visible mark explaining its origin. The legend will mention the AI model used and the main instruction given to it. The Explorations of Heidi.news (big stories, investigations, reports) are not intended to be illustrated by synthetic images, unless these creations have been designed by an artist using AI under his own responsibility.",08-04-2023,https://www.heidi.news/cyber/la-redaction-de-heidi-news-prend-position-sur-l-usage-des-intelligences-artificielles
Europe,United Kingdom,BBC,PBC,Scaling responsible machine learning at the BBC,"The BBC's Values: The BBC's ML engines will reflect the values of our organization, upholding trust, putting audiences at the heart of everything we do, celebrating diversity, delivering quality and value for money, and boosting creativity.

Our Audiences: Our audiences create the data which fuels some of the BBC's ML engines, alongside BBC data. We hold audience-created data on their behalf and use it to improve their experiences with the BBC. Audiences have a right to know what we are doing with their data. We will explain, in plain English, what data we collect and how this is being used, for example in personalization and recommendations.

Responsible Development of Technology: The BBC takes full responsibility for the functioning of our ML engines (in house and third party). Through regular documentation, monitoring, and review, we will ensure that data is handled securely and that our algorithms serve our audiences equally and fairly so that the full breadth of the BBC is available to everyone.

Incorporating the BBC's editorial values and seeking to broaden, rather than narrow horizons: Where ML engines surface content, outcomes are compliant with the BBC's editorial values (and where relevant as set out in our editorial guidelines). We will also seek to broaden, rather than narrow, our audience's horizons.

Continued innovation and human in the loop oversight: ML is an evolving set of technologies, where the BBC continues to innovate and experiment. Algorithms form only part of the content discovery process for our audiences and sit alongside (human) editorial curation.

The Datalab team is currently testing this approach as they build the BBC's first in-house recommender systems, which will offer a more personalized experience for BBC Sport and BBC Sounds. We also hope to improve the recommendations for other products and content areas in the future. We know that this framework will only be impactful if it is easy to use and can fit into the workflows of the teams building machine learning products.

The BBC believes there are huge benefits to being transparent about how we're using Machine Learning technologies. We want to communicate to our audiences how we're using their data and why. We want to demystify machine learning. And we want to lead the way on a responsible approach. These factors are not only essential in building quality ML systems but also in retaining the trust of our audiences.

This is only the beginning. As a public service, we are ultimately accountable to the public and so are keen to hear what you think of the above.

In 2020, BBC staff across data science, engineering, research and development, policy, legal, and product came together to develop a checklist aid to putting MLEP into practice.

The MLEP Checklist sections are designed to correspond to each stage of developing an ML project and contain prompts which are specific and actionable. Not every question in the checklist will be relevant to every project, and teams can answer in as much detail as they think appropriate. We ask teams to agree and keep a record of the final checklist; this self-audit approach is intended to empower practitioners, prompting reflection and appropriate action.

When pulling the checklist together we sought to draw from best practice within the BBC and the wider industry. Internal teams also piloted several iterations of the checklist and provided feedback.

We're seeing the MLEP approach have real impact in bringing on board stakeholders from across the organization, prompting thinking about important issues like transparency, diversity, and privacy in ML systems and helping teams anticipate and tackle issues early in the development cycle.",01-10-2019,https://www.bbc.co.uk/blogs/internet/entries/4a31d36d-fd0c-4401-b464-d249376aafd1
Europe,United Kingdom,OFCOM,Regulator,Note to Broadcasters: Synthetic media (including deepfakes in broadcast programming),"“Synthetic media” is an umbrella term for video, image, text, or voice that has been generated in whole or in part by artificial intelligence algorithms. Synthetic media has become increasingly prevalent online and has also been used in virtual reality, augmented reality, gaming, and other forms of digital media. It is also used in marketing, advertising, and the entertainment industry, including in filmmaking and broadcasting. As this type of technology continues to grow and evolve at a rapid rate, synthetic media is likely to become more prevalent in broadcast content. 

In line with the right to freedom of expression we recognise the benefits such technologies can bring to both broadcasters and audiences. There are clear benefits to broadcasters who can use this technology, for example the potential to increase audience engagement through the creation of content that would be difficult or impossible to achieve with traditional media. However, the use of this technology also poses challenges to broadcasters and potential risks to audiences. For example, “DeepFake” software can replace a person in an existing image or video with someone else’s likeness with realistic results. The use of ‘DeepFake’ software has grown exponentially and it poses a number of challenges for broadcasters, these include: 

1. Misinformation and disinformation: synthetic media could be used to create fake news, propaganda and other forms of disinformation that can spread quickly online leading to challenges for broadcast journalists in authenticating footage from online sources.
2. Trust and credibility degradation: with the rise of deepfakes and other synthetic media, audiences may find it difficult to trust the authenticity of content and audiences could potentially be harmed if it is not apparent they are watching footage that is ‘Deepfake’.
3. Fairness and Privacy: audiences could mistake ‘Deepfake’ footage of a real person in a way that could result in unfairness to them or potentially unwarrantably infringe their privacy. 

Ofcom is confident that the existing rules within the Broadcasting Code will protect audiences from the potential harms that might arise through the use of synthetic media. For example, depending on the context, the relevant rules could include (but would not be limited to): 

1. Section Two (Harm and Offence): rules to protect audiences from potential harm and offence and from material which may be materially misleading (Rules 2.1, 2.2 and 2.3);
2. Section Five (due impartiality and due accuracy): ensures news content is reported with due accuracy (Rule 5.1); 
3. Section Seven (Fairness): includes a set of practices which ensure that broadcasters avoid unjust or unfair treatment of individuals or organisations in programmes; and Published on 3 April 2023 
4. Section Eight (privacy): includes a set of practices which ensure that broadcasters avoid any unwarranted infringement of privacy in programmes and in connection with obtaining material included in programmes. 

Ofcom emphasises that, consistent with broadcasters’ right to freedom of expression and audiences’ right to receive information and ideas, it is fundamental that both broadcasters and audiences can explore new and emerging technologies – including synthetic media - as they become an increasing part of our daily lives. However, Ofcom would like to remind all its licensees of their ongoing responsibility to comply with the Broadcasting Code in order to protect audiences from harm and maintain the high levels of trust in broadcast news as well as to ensure individuals and organisations are not treated unfairly and/or their privacy is not unwarrantably infringed. 

We would therefore advise all licensees to consider carefully whether their compliance processes need to be adapted or developed to account for the potential risks involved in the use of synthetic media technologies to create broadcast content.",03-04-2023,https://www.ofcom.org.uk/__data/assets/pdf_file/0028/256339/Note-to-Broadcasters-Synthetic-media-including-deepfakes-.pdf
Europe,Czech,Czech News Agency,Agency,,,,
Europe,Austria,APA,Agency,Guideline for dealing with Artificial Intelligence,"The Journalism Council has approved a new guideline on the use of artificial intelligence (AI) in journalism. It addresses the responsibility of (lead) editors and transparency to the public in the use of AI, both in the production of articles and in their dissemination. At the same time, the Council included a new article on transparency in the Code.
With the new directive, the Journalism Council is among the pacesetters in Europe to address the use of artificial intelligence in its Code. The Council's exchange of views began two years ago and has been completed just in time as the public debate over ChatGPT and other AI applications rages on.
Combined with the new article on transparency, the directive on AI reads as follows.

Article 12. The journalist is transparent about his assignment, approach and way of working. To the extent possible and to the extent relevant, he communicates this clearly to his audience.

Guideline to Article 12. Artificial intelligence can play a role in the cooking, editing, production and dissemination of news items such as articles, reports, illustrations, infographics, etc. Editorial choices play a role in such partially or fully automated processes. Those choices must comply with the principles of the Code.

The editors are responsible for these editorial choices, with final responsibility resting with the chief editors. The chief editors guarantee the principles of the Code in the development of systems that are partially or entirely driven by artificial intelligence. It monitors the application and implementation of these principles with respect to system developers. It is responsible at all times for the provision of information, regardless of how it is produced and regardless of the channel or form in which it is provided.
Editors communicate transparently about automated news production and personalization of news offerings, so that it is clear to users when news items are created or selected based on artificial intelligence.

1. Editors indicate when a news item or an element of the information offer is produced in part or in full based on automated processes and, to the extent possible, refer to the sources on which the item is based.
2. The editors shall indicate when a component of the information offer is selected or diversified based on profile or media consumption
",01-03-2022,https://apa.at/wp-content/uploads/2022/05/APA_KI_Leitlinie_BASIC_ES.pdf
Europe,Belgium,Raad voor de Journalistiek (RVDJ),NGO / Coalition,New guideline on the use of Artificial Intelligence in journalism,"The Journalism Council has approved a new guideline on the use of artificial intelligence (AI) in journalism. It addresses the responsibility of (lead) editors and transparency to the public in the use of AI, both in the production of articles and in their dissemination. At the same time, the Council included a new article on transparency in the Code. With the new directive, the Journalism Council is among the pacesetters in Europe to address the use of artificial intelligence in its Code. The Council's exchange of views began two years ago and has been completed just in time as the public debate over ChatGPT and other AI applications rages on.

Combined with the new article on transparency, the directive on AI reads as follows: Article 12: The journalist is transparent about his assignment, approach, and way of working. To the extent possible and to the extent relevant, he communicates this clearly to his audience.

Guideline to Article 12: Artificial intelligence can play a role in the cooking, editing, production, and dissemination of news items such as articles, reports, illustrations, infographics, etc. Editorial choices play a role in such partially or fully automated processes. Those choices must comply with the principles of the Code. The editors are responsible for these editorial choices, with final responsibility resting with the chief editors. The chief editors guarantee the principles of the Code in the development of systems that are partially or entirely driven by artificial intelligence. It monitors the application and implementation of these principles with respect to system developers. It is responsible at all times for the provision of information, regardless of how it is produced and regardless of the channel or form in which it is provided.

Editors communicate transparently about automated news production and personalization of news offerings, so that it is clear to users when news items are created or selected based on artificial intelligence. Editors indicate when a news item or an element of the information offer is produced in part or in full based on automated processes and, to the extent possible, refer to the sources on which the item is based. The editors shall indicate when a component of the information offer is selected or diversified based on profile or media consumption.",21-03-2023,https://www.rvdj.be/nieuws/nieuwe-richtlijn-over-het-gebruik-van-artificiele-intelligentie-de-journalistiek
Europe,Sweden / Global,United Robots,Innovation company,Generative AI in journalism: Our take on ChatGPT et al,"“What do you think of ChatGPT in the context of journalism? Saviour or enemy? Will AI make or break the news industry?” This is certainly the question du jour in our industry. We suggest it’s the wrong question. After seven years of providing automated articles to newsrooms (using a different type of AI), at United Robots, we’ve heard it all before. The fears of robots stealing jobs, of factually incorrect, untrustworthy content written in robotic language… It turns out that – surprise, surprise – reality is never as black-and-white as fears suggest. And in the case of this newer, generative AI (used in e g ChatGPT) – from where we stand – the scope is at once immense and limited. So, rather than focus on “saviour or enemy”, let’s take a step back and ask the question: “What can generative AI do for journalism, and what can’t it do?” And – most importantly – what role should people play in this process? Publishers are in the driver's seat ChatGPT is just a tool – albeit a brand new, powerful tool with huge scope, but a tool nonetheless. It does not change the guiding principles of journalism – a fundamentally human activity. Of course this type of AI can be used for nefarious ends, but so could the printing press. We are in the business of journalism and we should work out how the new tools can help us do that even better – as well as identify what risks may be involved. In mid January (2023), Futurism broke a story that perfectly illustrates the latter. Publisher CNET is using AI to write short financial articles, but has not been open about it. Some of the aspects of this story shine a bright light on the choices publishers have, irrespective of what type of AI they use: Transparency. We always recommend that AI written articles have a byline which makes it unequivocally clear that it was written by a robot, not a reporter. Transparency is critical internally as well as externally, and key for trust. In the case of the CNET story, the Verge reports that there seems to be a lack of transparency around the actual purpose of the content too. According to the Verge, the business model of CNETs relatively new owners Red Ventures, is about creating content designed to get high rankings in search, and then monetise the traffic. Their business model is not publishing journalism for people. Accuracy. It goes without saying that any content published within a journalistic platform needs to be correct and reliable – whether it’s a groundbreaking investigative piece by a seasoned journalist or a small text about a local football match or financial news. AI tools always need to be controlled by journalists. And if you’re going to auto publish AI generated texts, you cannot use generative AI tools like GPT-3 / ChatGPT – see explanation in fact box (right). Trust. The issue of trust really encompasses both of the above. Trust is the currency of journalism. Any deployment of new tech tools must in no way leave room for people to question the integrity of a publication. Having said that, we’ve found that readers are generally happy to embrace robot written content – as long as the information is valuable to them, and clearly labelled. If a publisher asked “What does generative AI mean for our business?”, we’d like to ask back: “What do you want it to mean? The AI is not in control, you are.” We would advise publishers to keep focussing on delivering solid, valuable journalism and use generative AI tools where they are helpful in this mission. Charlie Beckett, director of the JournalisAI project at LSE expressed it perfectly in a podcast recently, saying that these tools cannot ask critical questions or work out the next step in investigating a story, but that they can be a support to journalists in doing this work. “But I think it’s even more interesting how it puts a kind of demand on those journalists, saying ok – you’ve got to be better than the machine – you can’t just do routine, formulaic journalism anymore, because the software can do that.” We’re only at the beginning of exploring how generative AI can support the business of journalism. Trying out ChatGPT is easy – working large language models into robust and useful processes within a publishing business will be considerably harder. It will be crucial to keep a razor-sharp focus on the use you’re trying to extract from the tech and not get sidetracked by its inherent capabilities. At United Robots, we’re testing a number of possible uses for large language models, including prompting them to turn text into structured data (our “raw material”), also attempted elsewhere. It’s early days, there are lots of opportunities, and the measurable use and value we can derive from this tech is what will ultimately determine how we deploy it. Good journalism is about people – those who produce it and those who consume it. It’s about the unique work and voices of great reporters, something that can’t be replaced by ChatGPT. It’s about meeting the needs and expectations of readers, in a way that differentiates yours from other publications. Large language models are not able to work out what your unique product should be. AI can help improve our work processes, but it cannot produce journalism. Let’s not have an identity crisis. Rules based AI. In terms of generating automated fact based text, you can only really gain efficiencies by using rules based AI (at least at this point in time). With the United Robots platform (left in illustration) – and our model where we sell content rather than deploy tech – the all-important human(s) in the loop are involved before the text generation. The raw material is structured data from reliable sources. Our team programmes the robot. Word, clause and sentence alternatives are also written by humans. As well as translated if we’re building for a new market. When a new publisher is onboarded, this is also where they are involved. Our language team helps editors train the robot to the editorial guidelines and needs of their particular newsroom. Once all that is done, the humans’ work is done and the robot automatically writes and distributes texts to whatever end points the publisher needs. With the human in the loop before text generation you can create volumes of texts with guaranteed accuracy – as long as the data is correct. The content is scheduled, and publication predictable and can be automatic. Generative AI. By comparison, to the right in the illustration is what generative AI looks like for editorial quality text generation. The human in the loop needs to be involved in two sets of processes. Someone needs to create the prompts to drive the text generation, as well as optimise prompts to refine the outcome. Even after that, in order to guarantee accuracy and editorial quality, you also need to have someone check the facts, sources, conclusion etc in any AI generated text. At this point it's hard see efficiency gains with generative AI for automated text generation.",22-03-2023,https://www.unitedrobots.ai/generative-ai-in-journalism
Europe,Spain / Global,Prodigioso Volcán,Innovation company,AI for journalists: A tool to exploit,,01-11-2020,https://www.prodigiosovolcan.com/sismogramas/ia-periodistas/
North America,USA,Adobe,Multimedia software company,Adobe Generative AI Beta User Guidelines,"These Guidelines have two goals: to maintain the high quality of creative content generated using Adobe’s suite of products and services, and to keep our products and services accessible to our users in an engaging and trustworthy way that fosters creativity. Be Respectful and Safe Do not use Adobe’s generative AI features to attempt to create, upload, or share abusive, illegal, or confidential content. This includes, but is not limited to, the following: Pornographic material or explicit nudity Hateful or highly offensive content that attacks or dehumanizes a group based on race, ethnicity, national origin, religion, serious disease or disability, gender, age, or sexual orientation Graphic violence or gore. The promotion, glorification, or threats of violence. Illegal activities or goods. Self-harm or the promotion of self-harm. Depictions of nude minors or minors in a sexual manner. Promotion of terrorism or violent extremism. Dissemination of misleading, fraudulent, or deceptive content that could lead to real-world harm. Personal or private information of others (like full name, home address, phone number, email address, government issued IDs, or any other information relating to an identifiable individual). Be Authentic We disable accounts that engage in behavior that is deceptive or harmful, including: Using fake, misleading, or inaccurate information in your profile Impersonating other people or entities Using automated or scripting processes (such as bulk or automated uploading of content through a script) Engaging in schemes or third-party services to boost account engagement (artificially increasing the number of appreciations, views, or other metrics) Be Respectful of Third-Party Rights Using Adobe’s generative AI features to create content that violates third-party copyright, trademark, or other rights is prohibited. This may include, but is not limited to, entering text prompts to generate a third-party brand logo or uploading an input image that includes a third party’s copyrighted content. If you’re not sure whether your content violates the rights of a third party, you may want to reach out to an attorney or consult publicly available reference materials at the following: U.S. Copyright Office, U.S. Patent, Trademark Office AND Lumen If you want to report the misuse of your own creative work or your own intellectual property by one of our users, you can do that here: Intellectual Property Removal Policy located at https://www.adobe.com/legal/dmca.html. If you have a contract or other dispute with an Adobe user regarding content they have uploaded to our site, please resolve the issue directly with the user. We can’t moderate contract, employment, or other disputes between our users and the public. No Commercial Use While generative AI features are in beta, all generated output is for personal use only and cannot be used commercially.",08-05-2023,https://www.adobe.com/legal/licenses-terms/adobe-gen-ai-user-guidelines.html
North America,USA,WIRED,Online multimedia company,How WIRED Will Use Generative AI Tools,"LIKE PRETTY MUCH everyone else in the past few months, journalists have been trying out generative AI tools like ChatGPT to see whether they can help us do our jobs better. AI software can’t call sources and wheedle information out of them, but it can produce half-decent transcripts of those calls, and new generative AI tools can condense hundreds of pages of those transcripts into a summary.

Writing stories is another matter, though. A few publications have tried—sometimes with disastrous results. It turns out current AI tools are very good at churning out convincing (if formulaic) copy riddled with falsehoods.

This is WIRED, so we want to be on the front lines of new technology, but also to be ethical and appropriately circumspect. Here, then, are some ground rules on how we are using the current set of generative AI tools. We recognize that AI will develop and so may modify our perspective over time, and we’ll acknowledge any changes in this post. We welcome feedback at mail@wired.com.

Text Generators (e.g. LaMDA, ChatGPT)
We do not publish stories with text generated by AI, except when the fact that it’s AI-generated is the whole point of the story. (In such cases we’ll disclose the use and flag any errors.) This applies not just to whole stories but also to snippets—for example, ordering up a few sentences of boilerplate on how Crispr works or what quantum computing is. It also applies to editorial text on other platforms, such as email newsletters. (If we use it for non-editorial purposes like marketing emails, which are already automated, we will disclose that.)

This is for obvious reasons: The current AI tools are prone to both errors and bias, and often produce dull, unoriginal writing. In addition, we think someone who writes for a living needs to constantly be thinking about the best way to express complex ideas in their own words. Finally, an AI tool may inadvertently plagiarize someone else’s words. If a writer uses it to create text for publication without a disclosure, we’ll treat that as tantamount to plagiarism.

We do not publish text edited by AI either. While using AI to, say, shrink an existing 1,200-word story to 900 words might seem less problematic than writing a story from scratch, we think it still has pitfalls. Aside from the risk that the AI tool will introduce factual errors or changes in meaning, editing is also a matter of judgment about what is most relevant, original, or entertaining about the piece. This judgment depends on understanding both the subject and the readership, neither of which AI can do.

We may try using AI to suggest headlines or text for short social media posts. We currently generate lots of suggestions manually, and an editor has to approve the final choices for accuracy. Using an AI tool to speed up idea generation won’t change this process substantively.

We may try using AI to generate story ideas. An AI might help the process of brainstorming with a prompt like “Suggest stories about the impact of genetic testing on privacy,” or “Provide a list of cities where predictive policing has been controversial.” This may save some time and we will keep exploring how this can be useful. But some limited testing we’ve done has shown that it can also produce false leads or boring ideas. In any case, the real work, which only humans can do, is in evaluating which ones are worth pursuing. Where possible, for any AI tool we use, we will acknowledge the sources it used to generate information.

We may experiment with using AI as a research or analytical tool. The current generation of AI chatbots that Google and Microsoft are adding to their search engines answer questions by extracting information from large amounts of text and summarizing it. A reporter might use these tools just like a regular search engine, or to summarize or trawl through documents or their own interview notes. But they will still have to go back to the original notes, documents, or recordings to check quotes and references. In this sense, using an AI bot is like using Google Search or Wikipedia: It might give you initial pointers, but you must follow the links back to the original sources.

In practice, though, AI will make mistakes and miss things that a human would find relevant—perhaps so much so that it doesn’t save any time. Even if these tools do prove useful, we won’t want our reporters to rely on them any more than we’d let them rely on the limited information on Wikipedia. We’ll continue to insist on the same standards of research and original reporting as always. We also know that there are many professionally published research databases out there that come with lawful and highly accurate text- and data-mining tools, so we will constantly evaluate whether those meet our needs.

Image Generators (e.g. Dall-E, Midjourney, Stable Diffusion)
We may publish AI-generated images or video, but only under certain conditions. Some working artists are now incorporating generative AI into their creative process in much the same way that they use other digital tools. We will commission work from these artists as long as it involves significant creative input by the artist and does not blatantly imitate existing work or infringe copyright. In such cases we will disclose the fact that generative AI was used.

We specifically do not use AI-generated images instead of stock photography. Selling images to stock archives is how many working photographers make ends meet. At least until generative AI companies develop a way to compensate the creators their tools rely on, we won’t use their images this way.

We or the artists we commission may use AI tools to spark ideas. This is the visual equivalent of brainstorming—type in a prompt and see what comes up. But if an artist uses this technique to come up with concepts, we will still require them to create original images using their normal process, and not merely reproduce what the AI suggested.",17-03-2023,https://www.wired.com/about/generative-ai-policy/
North America,USA,News Media Alliance,NGO / Coalition,News/Media Alliance AI Principles,"As generative artificial intelligence (GAI) technologies become more prevalent, our membership believes these new tools must only be developed respecting journalistic and creative content, in accordance with principles that protect publishers’ intellectual property (IP), brands, reader relationships, and investments. The unlicensed use of content created by our companies and journalists by GAI systems is an intellectual property infringement: GAI systems are using proprietary content without permission. It’s also critical to acknowledge the societal risks associated with the proliferation of mis- and dis-information through GAI, which high-quality, original content, produced by skilled humans and trusted brands, can help to combat.

GAI developers and deployers must negotiate with publishers for the right to use their content in any of the following manners:

Training: Including publishers’ content in datasets and using it for GAI system training and testing.
Surfacing: The serving of publishers’ content in response to user inputs, possibly including a cover note generated by the GAI system of what is contained in the surfaced content.
Synthesizing: Summaries, explanations, analyses etc. of source content in response to a query.
This document highlights the overarching principles that must guide the development and use of GAI systems as well as the policies and regulations governing them. These principles are founded on our understanding of these systems and technologies as they are currently used – and may therefore be amended as these technologies and uses develop – and apply equally to all publisher content, whether in text, image, audiovisual or any other format.

AI Principles
Intellectual Property. Developers and deployers of GAI must respect creators’ rights to their content. These rights include copyright and all other legal protections afforded to content creators and owners, as well as contractual restrictions or limitations imposed by publishers for the access and use of their content (including through their on-line terms of service). Developers and deployers of GAI systems—as well as legislators, regulators and other parties involved in drafting laws and policies regarding GAI—must maintain an unwavering respect for these rights and recognize the value of creators’ proprietary content. GAI developers and deployers should not use publisher IP without permission, and publishers should have the right to negotiate for fair compensation for use of their IP by these developers. Professional journalism is particularly valuable due to its reliability, accuracy, coherency and timeliness, enhancing GAI system outputs and improving perceptions of system quality. Absent permission and specific licenses, GAI systems are not simply using publishers’ content, they are stealing it.

Use of publishers’ IP requires explicit permission. Use of publisher content by GAI systems for training, surfacing and synthesizing is not authorized by most publishers’ terms and conditions, and authorization for search should not be construed as an authorization for uses such as training GAI systems or displaying more content than contemplated for or as used in traditional search.  GAI system developers and deployers should not be crawling, ingesting or using publishers’ proprietary content without express authorization; requiring publishers to opt out is not acceptable. Negotiating written, formal agreements is therefore necessary.  Industry standards should be developed to allow for automatic detection of permissions that distinguish among potential uses of crawled or scraped content.  These standards and usage agreements can also address other issues such as attribution, monetization, responsibility, and derivative uses.

Compensation agreements must account for harms GAI systems may cause publishers and the public. GAI system surfacing and synthesizing are providing much more proprietary content and information from the original sources than traditional search and often provide little or no attribution, and will exacerbate the growing trend toward zero-click, reducing or even eliminating value for publishers. GAI systems use publishers’ proprietary content to generate outputs that may replace their role in the consumer/information provider relationship. In addition to reducing traffic, this harms publisher brands that have taken years, decades, or even centuries to build.

Copyright laws must protect, not harm, content creators. The fair use doctrine does not justify the unauthorized use of publisher content, archives and databases for and by GAI systems.  Any previous or existing use of such content without express permission is a violation of copyright law. The Section 1201 triennial rulemaking process should not be used to allow for the bypassing of content protections for GAI development purposes. Exceptions to copyright protections for text and data mining (TDM) should be narrowly tailored to limited nonprofit and research purposes that do not damage publishers or become pathways for unauthorized uses that would otherwise require permission.  The U.S. also has made international law commitments in this area that protect its IP-based businesses across multiple sectors and these must be upheld in its approach to AI.

There is an existing market for licensing publishers’ news content. Valuing publishers’ legitimate IP interests need not impede GAI innovation because compensation frameworks (for example, licensing) already exist to permit use in return for payment. GAI innovation should not come at the expense of publishers, but rather at the expense of developers and deployers.  Publishers encourage the use of efficient ways to license through standard-setting organizations that can facilitate efficient training of GAI systems.

Transparency. GAI systems should be transparent to publishers. Publishers have a right to know who copied our content and what they are using it for. We call for strong regulations and policies imposing transparency requirements to the extent necessary for publishers to enforce their rights. Publishers have a legitimate interest in determining what content of theirs has been and is used in GAI systems. Using datasets or applications developed by non-profit, research, or educational third parties to power commercial GAI systems must be clearly disclosed and not used to evade transparency obligations or copyright liability.

GAI systems should be transparent to users. Direct relationships between users and publishers are critical for the sustainability of the news media and informational content sector. Surfaced and synthesized outputs should connect, not disintermediate, users with publishers. Members of the public should know the source of information that may affect them.  Generative outputs should include clear and prominent attributions in a way that identifies to users the original sources of the output and encourages users to easily and directly navigate to those products, as well as to let them know when content is generated by GAI. Transparency into GAI systems can also help prevent misuse and the spread of mis- and dis-information. Similarly, it enables the evaluation of GAI systems for unintended bias to avoid discriminatory outcomes.

Accountability. Deployers of GAI systems should be held accountable for system outputs. GAI systems pose risks for competition, the integrity of news and creative content, and for public trust in the journalistic and creative content. This is aggravated by the ability of AI applications to devalue publisher brands by generating content that attributes false or inaccurate information to publishers who have not published the information and who have processes in place to prevent such publication in the first place. Accordingly, deployers of GAI systems should not be shielded from liability for their outputs—to do so would be to provide deployers of GAI systems with an unfair advantage against which traditional publishers cannot compete and increase the danger to the public and institutions from the unchecked power of this technology.

Fairness. GAI systems should not create, or risk creating, unfair market or competition outcomes. Regulators should be attuned to ensuring GAI systems are designed, deployed, and used in a way that is compliant with competition laws and principles. Developers and deployers should also use their best efforts to ensure that GAI models are not used for anti-competitive purposes. The use of publisher content for GAI purposes without express permission from content owners by firms that have market power in online content distribution should be considered evidence of a violation of competition laws.  Regulators should be vigilant for other anti-competitive uses of GAI systems.

Safety. GAI systems should be safe and avoid privacy risks. GAI systems, including GAI models, should be designed to respect the privacy of users who interact with them. Early indications are that GAI tools will exacerbate trends towards digital platforms collecting large volumes of user data. The collection and use of personal data in GAI system design, training and use should be minimal and should be disclosed to users in an easily understandable manner so that users can make informed judgments about how their data is used in exchange for the GAI service. Users should be informed about, and should have the right to prevent, the use of their interactions with GAI systems for the purposes of training or collection of personal data.  Systems should also be designed in a way that means paywalled and otherwise protected content cannot be exposed (including but not limited to, for example, by membership inference methods).

Design. All of the principles discussed above should be incorporated in the very design of GAI systems, as significant elements of the design, and not considered as an afterthought or a minor concern to be addressed when convenient or when a third party brings a claim.",20-04-2023,https://www.newsmediaalliance.org/ai-principles/
North America,USA,Partnership on AI,NGO / Coalition,PAI’s Responsible Practices for Synthetic Media A Framework for Collective Action,,19-02-2023,https://syntheticmedia.partnershiponai.org/#landing
North America,USA,Radio Television Digital News Association (RTDNA),NGO / Coalition,Use of Artificial Intelligence (AI) in Journalism,"AI (Artificial Intelligence) can have a role in ethical, responsible and truthful journalism. However, it should not be used to replace human judgment and critical thinking — essential elements of trusted reporting. It may have a role in your newsrooms as a tool to assist in your work.
If news organizations are going to use AI, RTDNA recommends they have a clear policy for how AI is to be used in newsgathering, editing and distributing content across platforms. AI intersects with core journalism principles like accuracy, context, trust, and transparency. Carefully weigh all issues before integrating into your news organization. Because this is an emerging and fast-changing area, newsrooms and RTDNA might find it necessary to review guidelines regularly.

Below is an outline of critical areas of focus and questions you should consider when drafting a policy:

Accuracy, Context and Clarity. AI programs have the ability to modify every element of content — audio, video, still pictures, and words.  In many cases, AI programs may enhance your media. However, AI programs may not offer the proper context, have facts misplaced or may be confusing to the end user without thoughtful guidelines.

The following questions should help you guide your decision-making around accuracy, context, and clarity: What do your newsroom/station/parent company guidelines say about AI use surrounding accuracy, context, and clarity? Are they up to date? Can you fully understand the capabilities and source material for the AI program before implementation? Also consider: What are your safeguards to protect against inadvertent plagiarism? Can you independently verify the AI tool’s accuracy?  Are there opportunities to test the AI tool prior to publication? How have you taken ownership over the disclosure language for the consumer? What is your newsroom system and set of expectations for human review before publication? Transparency and Disclosure.
In establishing policies around the use of artificial intelligence in newsrooms, consider the importance of transparency to the trust you build with your audience. 

In general, disclosing how you use artificial intelligence is preferable to non-disclosure. 

The following questions should help you guide your decision-making around transparency and AI use: Does the benefit to the public of your use of AI outweigh any risk or detriment to trust in news by not disclosing its use? How does your audience feel about the use of this technology? What falls under the definition of AI? Examples: content generation vs. content distribution/organization vs. video or text editing vs. grammar/spelling tools.Where will you provide disclosure about your use of AI? Examples: Website, social media, on-air
Are journalists able to review any and all AI-influenced content before it reaches the consumer? If not, can you justify its use to the audience? Privacy. Journalists have long-standing practices to weigh the public’s right to know against an individual’s right to privacy. It is unlikely AI can properly consider these issues in the same way. It is important to ensure that AI is programmed to operate within ethical and legal boundaries and that its use does not violate privacy or other fundamental rights.",17-05-2023,https://www.rtdna.org/news/rtdna-releases-coverage-guidelines-on-the-use-of-ai-in-journalism
North America,USA,CNET,Online multimedia company,"How We Will Use Artificial Intelligence at CNET: With proper guardrails, new tools can assist us in producing expert, unique and helpful advice.","CNET has built our expertise through more than 25 years of testing and assessing new technology. Our goal is to help drive conversations about how those advancements -- including AI -- can solve real-world problems, because we believe you can create a better future when you understand new ideas. Our commitment is to be transparent about our process and results and to deploy new tools in smart and ethical ways. Our ethical standard includes two tenets: One, every piece of content we publish is factual and original, whether it's created by a human alone or assisted by our in-house AI engine, which we call RAMP. (It stands for Responsible AI Machine Partner.) If and when we use generative AI to create content, that content will be sourced from our own data, our own previously published work, or carefully fact-checked by a CNET editor to ensure accuracy and appropriately cited sources. Two, creators are always credited for their work. The use of our AI engine will include training on processes that prioritize accurate sourcing and include standards of citation. CNET's use of AI is guided in part by a working group of staffers from across our organization who regularly meet to collaborate on principles that establish how we work with new and emerging tools. This page will keep you informed of that work, and we'll update it as we learn and adjust. As of this writing, here are some examples of places in our work where we are exploring leveraging RAMP. Organizing large amounts of information: After decades of reporting and testing products, we've got loads of valuable data -- and so do our trusted partners. One way we can use AI is to analyze and sort that information to create specific guidance, such as in geographically targeted reviews of local services. RAMP will help us sort things like pricing and availability data and present it in ways that tailor information to certain audiences. Without an AI assist, this volume of work wouldn't be possible. Speeding up certain research and administrative portions of our workflow: CNET editors could use AI to help automate some portions of our work so we can focus on the parts that add the most unique value. This might include creating outlines of a given topic or analyzing written work to let us know if it's missing key themes or relevant context we can add. RAMP may also generate content such as explanatory material (based on trusted sources) that a human could fact-check and edit. Here are situations where we won't being using AI: Writing full stories: None of the stories on CNET have been or will be completely written by an AI. If that changes, as technology and our processes evolve, we will disclose it here. For now, articles may contain portions of text that were generated by AI and then edited and fact-checked by our editors. Product testing or reviews: We are not using AI tools to do any of the hands-on, product-based testing that informs our reviews and ratings. Those tests are performed by our trusted, experienced and award-winning human experts, many of whom have a decade or more of experience with the tech they review. See here for more information about how we test everything. Generating images and videos: CNET staff photographers and video producers are evaluating approaches and best practices regarding the use of the generative AI to create images and videos. As of now, CNET is not publishing AI-generated images except as examples of AI capabilities in our coverage of tools currently on the market. On any stories or pages that feature text that originated from our AI tool, we'll include that information in a disclosure -- typically, in a secondary byline at the top of the story. On any story that deals substantively with the topic of artificial intelligence, whether or not we used the technology in the creation of that story, we'll include a disclosure about our organization's use of AI that links to this page. We're going to continue testing AI tools and we expect to learn a ton about how we can use these tools to create even better content and advice for you. We'll continue to let you know how it goes.",06-06-2023,https://www.cnet.com/ai-policy/
North America,USA,Business Insider,Online multimedia company,Memo,"I've spent many hours working with ChatGPT, and I can already tell having access to it is going to make me a better global editor-in-chief for Insider.

Just in the past couple of weeks it helped me think about how and what I wanted to say in this memo, do casual background research for a post I assigned, brainstorm headline ideas, and prepare for a live interview. It read and summarized Alvin Bragg's indictment and statement of facts in his case against Donald Trump in moments. I fed it some of the episode titles of one of our most popular video series and asked for future episode ideas. (I sent them to one of our
executive producers and she said, “Holy moly! There are some really great ideas here. Thank you!”) I asked it to come up with ideas for trips for our travel reporters, asking it to make additional recommendations for other related places a half day's trip away.

My takeaway after a fair amount of experimentation with ChatGPT is that generative Al can make all of you better editors, reporters, and producers, too.

Do not use ChatGPT or other chatbots and versions of Al to write sentences that you put into your scripts or articles.

This may change in the future. But before we make that change, we are going to ask a pilot group of experienced producers, editors, and reporters to experiment with it as a word processing aid and report back to the rest of us. If you are interested in joining this pilot group,
please let me know. It will be exciting and important work for Insider.

Anyone joining this group will receive three important warnings:

Generative Al can introduce falsehoods into the copy it produces. Research it provided me for this memo was wrong, as I discovered when I fact-checked it. You cannot trust Generative Al as a source of truth. Doing so can lead to journalistic disaster. Al can also introduce bias into text it generates. When it comes to facts, generative Al should be viewed as a resource similar to Wikipedia or a factoid at the top of a Google search-results page: that is, a great starting point that helps you find more reliable sources. ChatGPT is a language generator that performs calculations to guess at the next best word. It doesn’t understand facts or meaning, and it doesn’t know whether an assertion is right or wrong, much less fair. It is not a journalist — you are. No matter the tool you use, Al or otherwise, journalists at Insider are ultimately responsible for the accuracy of their stories. Always verify your facts.

Generative Al may lift passages from other people's work and present it as original text. Do not plagiarize! Always verify originality. Best company practices for doing so are likely to evolve, but for now, at a minimum, make sure you are running any passages received from ChatGPT through Google search and Grammarly’s plagiarism search.

A third, less serious, warning is that text generated by Al can be dull and generic. Take what it gives you as a suggestion: something to rewrite into your own voice and in Insider’s style. Make sure you stand by and are proud of what you file.

For these reasons and others, generative Al is tricky to use as a text drafting tool, and that’s why we are limiting experimentation with using it in this way to a small pilot group. We're excited to hear what they learn. Already it’s obvious that ChatGPT can help a reporter bust through writer’s block and generate ideas for a lede, kicker, or transitions. Our suggestion to the pilot group will be to take copy and rewrite the output until they're satisfied with it.

But beyond that use case, now is absolutely the time for the rest of us to begin experimenting with this powerful yet poorly understood new technology.

Here are some ideas for how it may help you:

Use Al to generate outlines for your stories, or to help structure a post that you're struggling with. This could help a lot with writer’s block.

Save your editors precious time they are currently spending fixing typos and cleaning up copy. Ask Al to make suggested edits to your writing to make it more readable and concise. Please see below for an important caution on this use case.*

Use Al to suggest SEO-optimized headlines and meta descriptions.

Tell Al who you are planning to interview and what you hope to get out of it, and ask for interview question ideas. This also works well for prepping for panels and interviews.

Ask Al to explain tricky, unfamiliar concepts. (“What happens if the US fails to raise the debt ceiling?”)

Ask Al to summarize old news stories and suggest lessons that can be learned from them. For example: “How did John Edwards avoid conviction and what lessons should future prosecutors learn from it?”

*Do not put sensitive information, particularly sourcing details, into ChatGPT. The Al companies employ humans who can see conversations with their bots.

I encourage all of you to try all those prompts, and then try a million more of your own creation.",13-04-2023,https://twitter.com/maxwelltani/status/1646502450092822531/
North America,USA,AP,News agency,The future of augmented journalism: A guide for newsrooms in the age of smart machines,,22-02-2017,https://www.ap.org/assets/files/2017_ai_guide.pdf
North America,USA,USA TODAY NETWORK,Online multimedia company,Ethical Guidelines and Policy for Gannett Journalists Regarding AI-Generated or Assisted Content,"PURPOSE. AI (Artificial Intelligence) is emerging as a helpful tool in publishing. However, before using any AI-generated content, you must discuss the purpose and how it was produced with your editor. This policy aims to provide ethical guidelines for journalists using AI-generated content: whether written, visual, or audio to ensure that their reporting is transparent, accurate, fair, and accountable.
If you and your editor decide AI-generated or assisted content is acceptable or warranted, these are our guidelines.

TRANSPARENCY and DISCLOSURE. AI is not transparent about its own sources. Until sourcing transparency improves, journalists need to treat AI-gen content as they would “off the record” sources, meaning it can be used for ideas and leads, but we need to confirm whatever it tells us with an “on the record” source.

ACCURACY and VERIFICATION. If AI-assisted content is approved for publication, journalists must disclose the use of AI and its limitations to their audience. AI-generated content must be verified for accuracy and factuality before being used in reporting. Journalists must ensure that the content they use is accurate and free of errors.

EDITORIAL JUDGMENT. Journalists must maintain their professional judgment and decision-making abilities when using AI-generated content. They must ensure that the content they use meets our editorial standards and values.

FAIR USE. Journalists must comply with the legal and ethical considerations of fair use when using AI-generated content. They must ensure that they do not infringe on the intellectual property rights of others.

AI VISUAL TOOLS (Dall-E and Others). AI-generated visual content is illustrative, does not accurately represent a real situation, and is not an accurate rendition of a genuine moment or news situation. There should be an extremely high bar for the potential use of AI-generated visual content, requiring full disclosure to the viewer. Care should be taken around any content so photo-realistic that an average viewer could mistake it as a depiction of a real person or place. In general, these generated images have no place in daily or breaking “news” coverage. Any use of visual AI should be approved by a managing editor or higher, the photo and video director, or the standards editor.

PRIVACY and DATA PROTECTION. Journalists must ensure that the use of AI-generated content does not violate the privacy rights of individuals. They must ensure that the data used to generate content is collected and used in compliance with data protection laws.

BIAS and DISCRIMINATION. Journalists must be aware of any potential biases in the AI-generated content they use. They must ensure that the content they use does not discriminate against any individual or group based on race, ethnicity, religion, gender, sexual orientation, or any other characteristic.

SAFETY and SECURITY. Journalists must be mindful of the potential risks of using AI-generated content for public safety and security. They must ensure that the content they use does not contribute to disinformation or misinformation that may cause harm.

ACCOUNTABILITY and RESPONSIBILITY. Journalists must take responsibility for any errors or inaccuracies in the AI-generated content they use. They must be accountable to their audience and take corrective action if errors are found.

INCLUSIVITY and DIVERSITY. Journalists must ensure that they are inclusive and diverse in their use of AI-generated content. They must ensure that the content they use reflects the diversity of their audience.

TRAINING and DEVELOPMENT. Journalists must receive adequate training and development to use AI-generated content effectively and ethically. They must stay informed about the latest developments in AI technology and its ethical implications.

CONCLUSION. These ethical guidelines and policies aim to ensure that journalists use AI-generated content in a transparent, accurate, fair, and accountable manner. Journalists must adhere to these guidelines to maintain the highest standards of journalism and to maintain the trust of their audience. We want our journalists to embrace technology while being mindful of the pitfalls of leaning too heavily on this technology in their day-to-day work. The guidelines may be revised periodically to reflect the evolving ethical implications of AI (Artificial Intelligence) technology.",27-04-2023,https://cm.usatoday.com/ethical-conduct/
North America,Canada,Globe and Mail,Newspaper,A note on AI and The Globe and Mail newsroom,"Since ChatGPT was released to the world late last year, the advances in AI tools for working with text, images and video have continued at a blistering pace. Artificial intelligence will bring big changes to the way we work, think and collaborate as a newsroom. But we also have many questions about how these tools will influence the journalism we do.

As a first step in navigating these waters, we would like to lay out a few guiding principles to help with decision-making in the newsroom. This is a starting point, and will no doubt change as technology (and our thinking about it) evolves. Hopefully it will provide guardrails for your daily work, but please feel free to get in touch with any questions or feedback, or specific use cases.
In general, our Code of Conduct provides some good guidance:It is unacceptable to represent another person’s [or machine’s] work as your own. Information from another publication must be checked and credited before it is used. Illustrative and conceptual photography must be labelled “photo illustration.” So, treat ChatGPT and similar products as you would a tool like Wikipedia or a tip from a new source: be skeptical and always independently verify any facts presented. Ninety per cent accuracy might work for winning an argument over coffee, but would be a reputation killer for any journalist.

There is also a flood of new tools coming online every week – some with questionable ethics and opaque terms of use. If you would like to experiment with something beyond what is mentioned below, please reach out first. Also, any use of AI to work with confidential, proprietary or personal information must be first discussed with your manager and approved.
Here are some more specific guidelines: Research and reporting: AI language tools can be a great starting point for things like research, brainstorming and interview preparation. But any response from these tools should be treated with skepticism: it is unverified information, and AI is not concerned with truth in the same way we are as journalists. Writing: AI tools like ChatGPT should not be used to condense, summarize or produce writing for publication. Doing so would potentially risk our reputation and the confidentiality of our reporting. These tools may be used when writing about the topic of AI or adjacent technologies, as long as we are upfront with the reader (As Ian Brown did in this recent piece and the audience team did in this TikTok). ChatGPT and other tools like to impersonate the “human” voice. Let’s guard against any ambiguity on the source of AI text. Display writing and social media: AI tools like ChatGPT can be used to brainstorm ideas for headlines and social posts. However, any result should be vetted by an editor for accuracy before publication. Unedited drafts or unpublished stories should not be put through any AI tool before publication. Editing: AI tools should not be used to edit stories, as errors may be introduced in the process. Photography: AI image tools like Midjourney and DALL-E are not to be used for news photography. We will continue to follow our Code of Conduct on this matter: only basic image editing is permitted. Doing otherwise would risk our reputation and could draw into question the veracity of other images we publish. Feature illustrations and photo illustrations: AI tools may be used as part of the conception of feature illustrations, either as research or as part of a composite image. AI-generated visuals should not attempt to reproduce the likeness of a public person or brand. These works should always be credited as “photo illustration” or “illustration.” If an image is produced entirely by AI, it should be credited as “AI-generated image” or “AI-generated illustration.” (We should not assume readers are conversant in all of the AI tools available, so general labelling is best.) Finally, any work we create using AI must abide by that tool’s terms of use. Video: Similarly, AI video tools should not be used to create news video, except in cases where AI or similar technology is the subject of the video. The label “AI-generated video” should appear on screen at all times whenever an AI tool is used as part of a feature or news video. Code and data: AI tools can be used for prototyping, testing and debugging code, but we should not publish the final output without vetting and oversight. Any output must be independently verified before publication. Assume that the AI will occasionally get things wrong just as a human might. AI tools should not be used to process confidential, proprietary or personal information. If you have any question on a tool or process, please check with your manager before diving in.",14-06-2023,https://www.theglobeandmail.com/canada/article-a-note-on-ai-and-the-globe-and-mail-newsroom/
North America,Canada,Reuters,Agency,Data and AI ethics principles,"Thomson Reuters will adopt the following Data and AI Ethics Principles to promote trustworthiness in our continuous design, development, and deployment of artificial intelligence (""AI"") and our use of data. That Thomson Reuters use of data and AI are informed by our Trust Principles. That Thomson Reuters will strive to partner with individuals and organizations who share similar ethical approaches to our own regarding the use of data, content, and AI. That Thomson Reuters will prioritize security and privacy in our use of data and throughout the design, development, and deployment of our data and AI products and services. That Thomson Reuters will strive to maintain meaningful human involvement and design, develop, and deploy AI products and services and use data in a manner that treats people fairly. That Thomson Reuters aims to use data and to design, develop, and deploy AI products and services that are reliable, consistent, and empower socially responsible decisions. That Thomson Reuters will implement and maintain appropriate accountability measures for our use of data and our AI products and services. That Thomson Reuters will implement practices intended to make the use of data and AI in our products and services understandable. Thomson Reuters will use employee data to ensure a safe and inclusive work environment and to ensure employee compliance with regulations and company policies. We believe these Data and AI Ethics Principles will provide our colleagues and partners with the right foundations to build trustworthy, practical, and beneficial AI for our customers. These Data and AI Ethics Principles will evolve as the related industries continue to mature.",16-10-2022,https://www.thomsonreuters.com/en/artificial-intelligence/ai-principles.html
South America,Brazil,Nucleo,Online multimedia company,Policy for Artificial Intelligence,,18-05-2023,https://nucleo.jor.br/politica-ia/
Asia,Hong Kong,Hong Kong Free Press,Online multimedia company,HKFP CODE OF ETHICS,"As of 2023, news stories written with generative A.I. have been proven to introduce undeclared errors or “hallucinated” content, as well as produce biased, outdated or plagiarised content. Few A.I. tools include proper sourcing or attribution information, therefore HKFP will not currently be adopting generative A.I. for any news writing, image generation or fact-checking. Nor will we quote such bots as a source. Our audience must be assured that HKFP’s output is the work of our journalists and freelancers, and not a third-party technology. We may nevertheless use internally-approved A.I. tools for grammar/spell checking, for rewording/summarising existing text written by our team, and for assisting with translations, transcriptions, or for research. Specifically, we may make use of trusted A.I. tools that have been exclusively trained on HKFP’s archive, or may consult them for story/interview question ideas, or for assistance with coding, mathematical tasks or data crunching. Owing to the technology’s aforementioned limitations, we will confirm and double-check AI-generated output.",2023,https://hongkongfp.com/hkfp-code-ethics/
Asia,China,Cyberspace Administration of China,Regulator,Provisions on the Administration of Deep Synthesis of Internet-based Information Service,,25-11-2022,https://www.gov.cn/zhengce/zhengceku/2022-12/12/content_5731431.htm
Asia,Japan,The Japan Newspaper Publishers and Editors Association,NGO / Coalition,Opinion on the Use of News Content by Generative AI,,17-05-2023,https://www.pressnet.or.jp/statement/20230517_en.pdf
Asia,Korea,Korea Communications Commission,Regulator,Basic principles of AI-based media recommendation service user protection,,22-04-2023,https://www.korea.kr/archive/expDocView.do?docId=39940
Oceania,Australia,"Media, Entertainment and Arts Alliance (MEAA)",NGO / Coalition,DRAFT Position Statement on Artificial Intelligence,,09-03-2023,https://www.meaa.org/mediaroom/caution-and-consultation-needed-as-ai-is-rolled-out-in-media-and-entertainment/
,New Zealand,Stuff,Online multimedia company,Stuff Editorial Code of Practice and Ethics,"Artificial intelligence covers a broad range of technologies designed to replicate human intelligence and automate human processes. AI tools present opportunities to significantly enhance journalism and the experience for audiences, while improving productivity, but also create some risks. Stuff currently has internal teams considering AI developments and using AI tools. 

Transparency. Any content (written, visual or audio) generated or substantially generated using generative AI will be transparently labelled outlining the nature of AI use, including the tool used. 

Human oversight and fact checking. Any content generated by AI must meet the same standards of accuracy, fairness and balance as any other piece of content. The content will be held to the same standards as content written by professional journalists. Content must adhere to the requirements of New Zealand law and the principles of the New Zealand Media Council, of which Stuff is a member, and to Stuff's Editorial Code of Practice and Ethics.",15-06-2023,https://www.stuff.co.nz/about-stuff/300106664/stuff-editorial-code-of-practice-and-ethics
,,,,,,,
NOTE,,,,,,,
GUIDELINE,INCLUDE,,,,,,
OPINION,EXCLUDE,,,,,,
REGULATION,EXCLUDE,,,,,,
WAYBACK MACHINE,,,,,,,